{
 "metadata": {
  "name": "everything.me 2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classifying Movies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to build a classifier. We will first try to find some features that give a good contrast between the genres.\n",
      "\n",
      "To do that, we will first tokenize the words in the plot name"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Gameplan"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will try to use a naive bayessian classifier. But the main problem would be to choose the features.\n",
      "\n",
      "Possible features would be the words in the plot, the words in the title, combinations of words, length of description, year the moview came out, etc.\n",
      "\n",
      "We will try to use just the 1-grams (single words) first and see if they are good enough, since they are certainly the ones to hold the most information.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Loading the training set and tokenizing it"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import pprint\n",
      "from time import time\n",
      "import nltk\n",
      "from collections import defaultdict\n",
      "import pickle\n",
      "\n",
      "pp = pprint.PrettyPrinter(indent=6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_training_set(trainingFileName):\n",
      "    movies = []\n",
      "    vip_tokens = []\n",
      "    for line in open(trainingFileName):\n",
      "        movies.append(json.loads(line))\n",
      "    return movies"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "movies = load_training_set('../../movies_train.json')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "We need to tokenize and stem the 1-grams"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is a need to normalize some words (for example 'athlete','athletes' and 'athletics') in order to better group similar words to gain stronger and more stable statistics.\n",
      "\n",
      "Therefore we will use a stemmer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pos_tag_plots(movies, is_save=False):\n",
      "    stemmer = nltk.stem.snowball.EnglishStemmer()\n",
      "    n_movies = []\n",
      "    tic = time()\n",
      "    tokens = {}\n",
      "    pos_tags = {}\n",
      "    L=10000\n",
      "    for movie in movies:\n",
      "        n_movies.append(movie)\n",
      "        name = movie['name']\n",
      "        # we print out every once in a while, for a keepalive\n",
      "        if len(n_movies)%L == L-1:\n",
      "            print 'Finished processing ' + str(len(n_movies)) + ' movies'\n",
      "        n_movies[-1]['tokens'] = [stemmer.stem(token) for token in nltk.word_tokenize(movie['plot'].lower())]\n",
      "        if is_save:\n",
      "            with open('movies_tokenized.json','a') as fid:\n",
      "            json.dump(n_movies[-1],fid)\n",
      "            fid.write('\\n')\n",
      "    toc = time()\n",
      "    print 'Done with ' + str(len(n_movies)) + '! ' + str((toc-tic)/float(len(n_movies))) + ' seconds per movie'\n",
      "    return n_movies"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load (or save if does not exist) the tokenized data)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is sensible to maybe pursue a POS tagging strategy, and then use only verbs, for example, etc to catogorize some genres.\n",
      "\n",
      "However, it is not needed as a first iteration, and so we will stick with simple tokens (words...) for now, and apply POS tagging later, if needed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_tokenized_movies(n_movies):\n",
      "    for movie in n_movies:\n",
      "        with open('movies_tokenized.json','a') as fid:\n",
      "            json.dump(movie,fid)\n",
      "            fid.write('\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_tokenized_movies():\n",
      "    n_movies = []\n",
      "    cnt = 1\n",
      "    L=10000\n",
      "    try:\n",
      "        for line in open('movies_tokenized.json','r'):\n",
      "            if cnt%L == L-1:           \n",
      "                print \"loading line \" + str(cnt)\n",
      "            cnt+=1\n",
      "            n_movies.append(json.loads(line))\n",
      "    except:\n",
      "        return None\n",
      "    return n_movies"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_movies = load_tokenized_movies()\n",
      "if not n_movies:\n",
      "    n_movies = pos_tag_plots(movies, is_save = True)\n",
      "    save_tokenized_movies(n_movies)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished processing 9999 movies\n",
        "Finished processing 19999 movies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing 29999 movies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing 39999 movies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing 49999 movies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing 59999 movies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing 69999 movies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing 79999 movies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing 89999 movies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing 99999 movies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing 109999 movies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done with 111934! 0.00312685025035 seconds per movie"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(n_movies)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "111934"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Create a count of each token inside each genre"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We count a token that appears multiple times inside one movie once (notice the \"set\" operation for that).\n",
      "\n",
      "genres hold a structure such that we know for each genre, and each token, how many time it appeared.\n",
      "\n",
      "Meanwhile, for each token we also count in how many movies it appeared in total.\n",
      "\n",
      "We also count how many movies are in each genre.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "genres = {}\n",
      "tokens_count = defaultdict(int)\n",
      "genres_count = defaultdict(int)\n",
      "N_tokens = 0\n",
      "cnt = 1\n",
      "L = 10000\n",
      "for movie in n_movies:\n",
      "    if cnt%L == L-1:           \n",
      "        print 'Processed ' + str(cnt) + ' movies so far'\n",
      "    cnt+=1\n",
      "    for genre in movie['genres']:\n",
      "        genres_count[genre] += 1\n",
      "    for token in set(movie['tokens']):        \n",
      "        # the total count should be calculated such that if a word appeared more than once in a movie it is counted once.\n",
      "        # that way we can later normalize IDF by the number of movies\n",
      "    #    tokens_count_1_per_movie[token] += 1        \n",
      "    #for token in movie['tokens']:        \n",
      "        N_tokens += 1\n",
      "        tokens_count[token] += 1\n",
      "        for genre in movie['genres']:\n",
      "            if not genres.has_key(genre):\n",
      "                genres[genre]= {}\n",
      "            if not genres[genre].has_key(token):\n",
      "                genres[genre][token] = {'count':1, 'tf_in_genre':None , 'tf_tot':None, 'idf':None}\n",
      "            else:\n",
      "                genres[genre][token]['count'] += 1\n",
      "print 'done'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 9999 movies so far\n",
        "Processed 19999 movies so far"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processed 29999 movies so far"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processed 39999 movies so far"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processed 49999 movies so far"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processed 59999 movies so far"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processed 69999 movies so far"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processed 79999 movies so far"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processed 89999 movies so far"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processed 99999 movies so far"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processed 109999 movies so far"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Calculate Tf-Idf"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We recreate the classic TF-IDF in the sense that we want to know how much a token is frequent inside a genre, vs. how much it is frequent in the \"general\" populace.\n",
      "\n",
      "You can look at http://en.wikipedia.org/wiki/Tf%E2%80%93idf for a refresher"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We try two \"IDF\"s - one is log2(1/DF), where DF is the frequncy in the general populace.\n",
      "\n",
      "The second which we call \"IDF2\" is simply (1/DF)\n",
      "\n",
      "Note that we dont need to worry avout DF = 0 since we impose a threshold, both absolute (minimum appearance in the genre) and relative (minimum probability of appearance in the genre)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def delete_file(filename):\n",
      "    name = os.path.join('.', filename)\n",
      "    try:        \n",
      "        os.remove(name)\n",
      "        print 'old file deleted, ' + name\n",
      "    except:\n",
      "        print 'no previous file found ' + name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_tf_idf(filename,stats, genre):\n",
      "    with open(filename,'a') as fid:\n",
      "        for genre_word in stats[genre]:\n",
      "            fid.write(genre + ',' + genre_word[0] + ',')\n",
      "            json.dump(genre_word[1],fid)\n",
      "            fid.write('\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "WORD_REL_THR = 0.10\n",
      "WORD_ABS_THR = 10\n",
      "\n",
      "\n",
      "delete_file('tf-idf.txt')\n",
      "delete_file('tf-idf2.txt')\n",
      "\n",
      "tf_idf_stats = {}   \n",
      "tf_idf2_stats = {}\n",
      "\n",
      "for genre, count in genres.iteritems():\n",
      "    genre_list = []\n",
      "    for token in genres[genre]:           \n",
      "        genre_word = genres[genre][token]\n",
      "        if genre_word['count'] < WORD_ABS_THR:\n",
      "            continue\n",
      "        # calculate the frequncy of a token inside the genre\n",
      "        genre_word['tf_in_genre'] = round(float(genre_word['count']) / genres_count[genre], 5)\n",
      "        if genre_word['tf_in_genre'] > WORD_REL_THR:\n",
      "            # calculate the frequency of a token in all genres\n",
      "            genre_word['tf_tot'] = round(float(tokens_count[token]) / len(n_movies),3)\n",
      "            genre_word['idf'] = math.log(1.0/genre_word['tf_tot'],2)\n",
      "            genre_word['tf-idf'] = round(genre_word['idf'] * genre_word['tf_in_genre'],3)\n",
      "            genre_word['idf2'] = round(1.0/genre_word['tf_tot'],1)\n",
      "            genre_word['tf-idf2'] = round(genre_word['idf2'] * genre_word['tf_in_genre'],2)\n",
      "\n",
      "            #line = '\\tToken: ' + token + ', count: ' + str(genre_word['count']) +', tf: ' + str(genre_word['tf_in_genre']) + ', tf_tot: ' + str(genre_word['tf_tot']) + ', idf: ' + str(genre_word['idf']) + ', tf-idf: ' + str(genre_word['tf-idf'])\n",
      "            #print line\n",
      "            #fid.writelines( line + '\\n')\n",
      "            genre_list.append([token,genre_word])\n",
      "            \n",
      "    # note that we apply the generatos to make them into lists, because we want to use them several times...\n",
      "    tf_idf_stats[genre] = [x for x in reversed(sorted(genre_list, key=lambda x: x[1]['tf-idf']))]\n",
      "    tf_idf2_stats[genre] = [x for x in reversed(sorted(genre_list, key=lambda x: x[1]['tf-idf2']))]\n",
      "    \n",
      "    save_tf_idf('tf-idf.txt',tf_idf_stats, genre)\n",
      "    save_tf_idf('tf-idf2.txt',tf_idf2_stats, genre)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "old file deleted, ./tf-idf.txt\n",
        "old file deleted, ./tf-idf2.txt\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Finding the features that cover each genre (and hope it's enough)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets try to find, for each genre the list of high tf-idf tokens that spans all of it (or most)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SCORE_THR = 1.3\n",
      "genre_key_words = {}\n",
      "for genre in genres:\n",
      "    print 'Processing genre: ' + genre\n",
      "    genre_movies = [movie for movie in n_movies if genre in movie['genres']]\n",
      "        \n",
      "    for k in range(1,100):\n",
      "        topK = [(x[0],x[1]['tf-idf2']) for x in tf_idf2_stats[genre][:k]]\n",
      "        minimum_score = x[1]['tf-idf2']\n",
      "        if minimum_score < SCORE_THR:\n",
      "            finalK = k - 1\n",
      "            break\n",
      "    \n",
      "        cnt_yes = 0\n",
      "        cnt_no = 0\n",
      "        \n",
      "        for movie in genre_movies:\n",
      "            if genre in movie['genres']:\n",
      "                covered = False\n",
      "                for token in [x[0] for x in topK]:\n",
      "                    if token in movie['tokens']:\n",
      "                        covered = True\n",
      "                if covered:\n",
      "                    cnt_yes +=1\n",
      "                else:\n",
      "                    cnt_no +=1\n",
      "        finalK = k\n",
      "        coverage = cnt_yes / float(cnt_yes+cnt_no)\n",
      "        if coverage > 0.95:\n",
      "            break\n",
      "    genre_key_words[genre] = finalK,cnt_yes, cnt_no, minimum_score, coverage, topK\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing genre: Sci-Fi\n",
        "Processing genre: Crime"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Romance"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Animation"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Music"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Adult"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Comedy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: War"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Horror"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Film-Noir"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Adventure"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: News"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Thriller"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Western"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Mystery"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Short"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Lifestyle"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Drama"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Action"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Documentary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Musical"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: History"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Family"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Fantasy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing genre: Sport"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for genre, typical_words in genre_key_words.iteritems():\n",
      "    print genre, typical_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Sci-Fi (31, 4396, 228, 1.38, 0.9506920415224913, [(u'planet', 14.6), (u'alien', 12.47), (u'scientist', 10.84), (u'earth', 10.73), (u'space', 9.17), (u'human', 4.69), (u'futur', 4.67), (u'evil', 4.19), (u'power', 2.94), (u'mysteri', 2.69), (u'save', 2.15), (u'secret', 2.12), (u'discov', 2.12), (u'fight', 2.05), (u'group', 2.05), (u'world', 1.98), (u'must', 1.95), (u'use', 1.87), (u'forc', 1.83), (u'call', 1.76), (u'now', 1.57), (u'name', 1.55), (u'...', 1.47), (u'peopl', 1.46), (u'onli', 1.44), (u'can', 1.44), (u'kill', 1.4), (u'been', 1.4), (u'befor', 1.4), (u'into', 1.39), (u'begin', 1.38)])\n",
        "Crime (28, 8243, 422, 1.31, 0.9512983266012695, [(u'crimin', 5.79), (u'crime', 5.54), (u'detect', 5.42), (u'polic', 4.79), (u'investig', 3.76), (u'murder', 3.7), (u'gang', 3.4), (u'kill', 2.55), (u'money', 2.22), (u'plan', 1.88), (u'soon', 1.51), (u'get', 1.48), (u'name', 1.45), (u'wife', 1.45), (u'himself', 1.43), (u'him', 1.42), (u'forc', 1.42), (u'help', 1.42), (u'befor', 1.39), (u'out', 1.36), (u'turn', 1.36), (u'now', 1.36), (u'also', 1.36), (u'after', 1.34), (u'up', 1.34), (u'find', 1.32), (u'use', 1.31), (u'tri', 1.31)])\n",
        "Romance (26, 12673, 648, 1.41, 0.9513550033781247, [(u'marri', 2.83), (u'fall', 2.72), (u'love', 2.63), (u'meet', 2.19), (u'she', 1.84), (u'beauti', 1.83), (u'daughter', 1.8), (u'girl', 1.73), (u'her', 1.72), (u'doe', 1.69), (u'him.', 1.67), (u'woman', 1.66), (u'both', 1.63), (u'know', 1.6), (u'want', 1.59), (u'leav', 1.58), (u'return', 1.55), (u'each', 1.53), (u'father', 1.53), (u'soon', 1.52), (u'name', 1.51), (u'decid', 1.51), (u'get', 1.48), (u'him', 1.45), (u'also', 1.43), (u'young', 1.41)])\n",
        "Animation (7, 3923, 2879, 1.27, 0.5767421346662747, [(u'anim', 7.24), (u'littl', 2.05), (u'off', 1.58), (u'use', 1.49), (u';', 1.43), (u'(', 1.31), (u')', 1.31), (u'into', 1.27)])\n",
        "Music (19, 3195, 362, 1.26, 0.8982288445319089, [(u'music', 9.9), (u'song', 9.3), (u'band', 8.99), (u'sing', 8.12), (u'rock', 7.33), (u'singer', 7.16), (u'perform', 5.73), (u'danc', 4.78), (u'featur', 4.14), (u'play', 2.51), (u'includ', 2.28), (u'show', 1.97), (u\"'\", 1.75), (u\"''\", 1.58), (u'``', 1.58), (u'film', 1.53), (u'(', 1.52), (u')', 1.52), (u'also', 1.36), (u'this', 1.26)])\n",
        "Comedy (2, 11810, 19824, 1.27, 0.37333249035847504, [(u'get', 1.38), (u\"n't\", 1.37), (u'so', 1.27)])\n",
        "War (18, 3046, 263, 1.26, 0.9205197944998489, [(u'nazi', 12.9), (u'german', 12.0), (u'war.', 11.19), (u'soldier', 10.75), (u'war', 8.78), (u'armi', 8.37), (u'battl', 4.06), (u'dure', 3.67), (u'american', 3.46), (u'offic', 3.25), (u'fight', 2.57), (u'against', 2.16), (u'men', 2.13), (u'world', 2.13), (u'forc', 2.11), (u'stori', 1.42), (u'between', 1.38), (u'return', 1.37), (u'first', 1.26)])\n",
        "Horror (18, 6092, 849, 1.29, 0.8776833309321423, [(u'dead', 3.16), (u'mysteri', 2.84), (u'murder', 2.58), (u'kill', 2.29), (u'night', 2.2), (u'group', 2.1), (u'hous', 1.98), (u'death', 1.98), (u'begin', 1.85), (u'soon', 1.85), (u'discov', 1.78), (u'...', 1.68), (u'turn', 1.55), (u'must', 1.38), (u'start', 1.36), (u'into', 1.35), (u'them', 1.34), (u'befor', 1.34), (u'find', 1.29)])\n",
        "Film-Noir (20, 396, 19, 2.08, 0.9542168674698795, [(u'suspect', 7.47), (u'murder', 6.23), (u'detect', 5.9), (u'killer', 5.77), (u'polic', 5.46), (u'crime', 5.26), (u'investig', 4.67), (u'prison', 4.37), (u'kill', 3.87), (u'involv', 3.79), (u'money', 2.96), (u'believ', 2.89), (u'plan', 2.78), (u'wife', 2.74), (u'husband', 2.63), (u'night', 2.62), (u'her.', 2.59), (u'him.', 2.58), (u'soon', 2.09), (u'...', 2.08)])\n",
        "Western (31, 4105, 208, 1.67, 0.9517737073962439, [(u'rancher', 24.81), (u'cattl', 21.94), (u'outlaw', 21.35), (u'ranch', 19.77), (u'sheriff', 16.03), (u'gold', 10.21), (u'hors', 8.93), (u'gang', 8.02), (u'indian', 7.64), (u'ride', 6.55), (u'town', 4.25), (u'owner', 3.92), (u'arriv', 3.75), (u'kill', 3.2), (u'men', 3.08), (u'head', 2.81), (u'murder', 2.67), (u'plan', 2.67), (u'money', 2.54), (u'escap', 2.45), (u'learn', 2.43), (u'brother', 2.26), (u'daughter', 2.18), (u'him.', 2.05), (u'fight', 2.03), (u'bring', 1.99), (u'return', 1.92), (u'off', 1.86), (u'then', 1.79), (u'been', 1.69), (u'him', 1.67)])\n",
        "News (19, 787, 53, 1.24, 0.9369047619047619, [(u'news', 8.96), (u'interview', 5.07), (u'documentari', 3.87), (u'our', 3.68), (u'explor', 3.2), (u'peopl', 2.48), (u'we', 2.44), (u'were', 2.3), (u'show', 2.29), (u'film', 2.09), (u'how', 1.88), (u\"'\", 1.79), (u'most', 1.66), (u'this', 1.49), (u'world', 1.45), (u'look', 1.42), (u'stori', 1.41), (u'follow', 1.37), (u'through', 1.35), (u'over', 1.24)])\n",
        "Thriller (33, 10201, 498, 1.36, 0.9534535937938126, [(u'investig', 3.38), (u'murder', 3.31), (u'polic', 2.83), (u'dead', 2.74), (u'mysteri', 2.61), (u'kill', 2.52), (u'secret', 2.11), (u'death', 2.0), (u'soon', 1.87), (u'seem', 1.81), (u'begin', 1.78), (u'discov', 1.76), (u'must', 1.66), (u'howev', 1.57), (u'call', 1.57), (u'wife', 1.55), (u'know', 1.54), (u'forc', 1.53), (u'befor', 1.52), (u'name', 1.51), (u'find', 1.49), (u'...', 1.49), (u'turn', 1.49), (u'now', 1.47), (u'been', 1.46), (u'start', 1.43), (u'himself', 1.39), (u'help', 1.39), (u'what', 1.38), (u'after', 1.38), (u'becom', 1.38), (u'tri', 1.37), (u'woman', 1.36)])\n",
        "Adult (16, 533, 27, 1.56, 0.9517857142857142, [(u'erot', 33.96), (u'sex', 17.0), (u'sexual', 15.17), (u'seduc', 13.82), (u'!', 4.64), (u'husband', 2.86), (u'women', 2.67), (u'girl', 2.1), (u'beauti', 2.01), (u'then', 1.9), (u'woman', 1.88), (u'anoth', 1.78), (u'her', 1.76), (u'she', 1.74), (u'final', 1.72), (u'some', 1.56)])\n",
        "Mystery (31, 4756, 226, 1.33, 0.9546366920915295, [(u'detect', 6.08), (u'investig', 5.38), (u'murder', 4.67), (u'mysteri', 4.66), (u'polic', 3.14), (u'dead', 2.77), (u'secret', 2.12), (u'death', 2.11), (u'kill', 2.09), (u'seem', 2.08), (u'discov', 1.99), (u'night', 1.8), (u'begin', 1.71), (u'soon', 1.64), (u'lead', 1.64), (u'call', 1.62), (u'find', 1.6), (u'befor', 1.58), (u'been', 1.57), (u'woman', 1.53), (u'had', 1.51), (u'know', 1.5), (u'was', 1.46), (u'name', 1.45), (u'...', 1.45), (u'there', 1.43), (u'man', 1.38), (u'help', 1.37), (u'tri', 1.34), (u'wife', 1.34), (u'what', 1.33)])\n",
        "Short (0, 4756, 226, 1.15, 0.9546366920915295, [(u'film', 1.15)])\n",
        "Lifestyle (0, 4756, 226, 1.15, 0.9546366920915295, [])\n",
        "Drama (12, 34148, 10005, 1.28, 0.7734015808665323, [(u'mother', 1.5), (u'son', 1.49), (u'father', 1.45), (u'wife', 1.42), (u'love', 1.4), (u'famili', 1.35), (u'young', 1.35), (u'woman', 1.32), (u'fall', 1.31), (u'she', 1.31), (u'leav', 1.3), (u'life', 1.3), (u'her', 1.28)])\n",
        "Action (33, 10037, 509, 1.37, 0.9517352550730135, [(u'gang', 3.36), (u'kill', 2.71), (u'polic', 2.7), (u'fight', 2.67), (u'against', 2.19), (u'forc', 2.04), (u'escap', 2.01), (u'name', 1.98), (u'murder', 1.98), (u'must', 1.95), (u'brother', 1.91), (u'plan', 1.9), (u'now', 1.8), (u'help', 1.62), (u'son', 1.61), (u'down', 1.57), (u'know', 1.53), (u'befor', 1.53), (u'daughter', 1.51), (u'been', 1.5), (u'also', 1.5), (u'set', 1.48), (u'use', 1.44), (u'himself', 1.43), (u'them', 1.42), (u'after', 1.41), (u'out', 1.41), (u'then', 1.4), (u'onli', 1.39), (u'return', 1.38), (u'find', 1.38), (u'him', 1.37), (u'get', 1.37)])\n",
        "Documentary (20, 14877, 777, 1.44, 0.9503641241855116, [(u'documentari', 5.94), (u'interview', 4.75), (u'explor', 3.22), (u'we', 2.59), (u'film', 2.56), (u'journey', 2.38), (u'these', 2.18), (u'most', 2.06), (u'peopl', 1.99), (u'mani', 1.93), (u'how', 1.84), (u'world', 1.73), (u'through', 1.72), (u'follow', 1.7), (u'this', 1.6), (u'stori', 1.58), (u'show', 1.55), (u'year', 1.5), (u':', 1.48), (u'was', 1.44)])\n",
        "Musical (28, 2902, 142, 1.45, 0.9533508541392904, [(u'sing', 8.66), (u'danc', 5.85), (u'music', 5.48), (u'marri', 2.75), (u'fall', 2.22), (u'love', 1.98), (u'name', 1.86), (u'show', 1.82), (u'daughter', 1.79), (u'meet', 1.79), (u'doe', 1.73), (u'want', 1.72), (u'girl', 1.7), (u'play', 1.62), (u'decid', 1.6), (u')', 1.57), (u'both', 1.56), (u'(', 1.56), (u'leav', 1.54), (u'son', 1.54), (u'get', 1.54), (u'soon', 1.51), (u'she', 1.51), (u'then', 1.49), (u'know', 1.48), (u'come', 1.46), (u'also', 1.46), (u'her', 1.45)])\n",
        "History (21, 2859, 258, 1.22, 0.917228103946102, [(u'histori', 6.31), (u'war', 3.61), (u'documentari', 2.73), (u'american', 2.55), (u'were', 2.47), (u'dure', 2.21), (u'stori', 1.98), (u'against', 1.96), (u'film', 1.76), (u'peopl', 1.68), (u'was', 1.66), (u'through', 1.5), (u\"'\", 1.48), (u'world', 1.48), (u'most', 1.46), (u'how', 1.45), (u'tell', 1.45), (u'year', 1.39), (u'this', 1.38), (u'first', 1.36), (u'between', 1.33), (u':', 1.22)])\n",
        "Family (14, 6306, 1577, 1.29, 0.7999492578967399, [(u'boy', 2.0), (u'littl', 1.96), (u'name', 1.53), (u';', 1.49), (u'famili', 1.45), (u'(', 1.42), (u'like', 1.42), (u')', 1.42), (u'home', 1.42), (u'show', 1.38), (u'then', 1.38), (u'get', 1.37), (u'help', 1.36), (u'so', 1.34), (u'use', 1.29)])\n",
        "Adventure (27, 6521, 533, 1.29, 0.9244400340232493, [(u'adventur', 4.9), (u'save', 2.33), (u'escap', 2.16), (u'fight', 2.08), (u'must', 1.8), (u'against', 1.78), (u'name', 1.71), (u'forc', 1.64), (u'kill', 1.63), (u'help', 1.63), (u'set', 1.6), (u'discov', 1.58), (u'off', 1.51), (u'them', 1.44), (u'now', 1.42), (u'world', 1.42), (u'return', 1.4), (u'use', 1.4), (u'befor', 1.39), (u'where', 1.37), (u'find', 1.36), (u'way', 1.36), (u'also', 1.36), (u'they', 1.34), (u'back', 1.32), (u'been', 1.31), (u'father', 1.3), (u'by', 1.29)])\n",
        "Fantasy (19, 4451, 560, 1.29, 0.8882458591099581, [(u'magic', 8.15), (u'evil', 5.12), (u'power', 2.67), (u'mysteri', 2.51), (u'must', 1.8), (u'world', 1.71), (u'name', 1.64), (u'discov', 1.53), (u'help', 1.44), (u'can', 1.44), (u'into', 1.4), (u'girl', 1.39), (u'where', 1.39), (u'which', 1.34), (u'back', 1.34), (u'now', 1.34), (u'himself', 1.32), (u'find', 1.31), (u'...', 1.31), (u'old', 1.29)])\n",
        "Sport (19, 1592, 208, 1.26, 0.8844444444444445, [(u'sport', 21.25), (u'coach', 20.92), (u'footbal', 19.72), (u'player', 13.58), (u'team', 9.18), (u'game', 7.0), (u'race', 5.64), (u'win', 5.19), (u'play', 2.63), (u'fight', 2.11), (u'against', 1.82), (u'world', 1.73), (u'most', 1.62), (u'show', 1.6), (u'follow', 1.58), (u'first', 1.45), (u\"'\", 1.43), (u')', 1.35), (u'(', 1.35), (u'some', 1.26)])\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Midway reflection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that some categories are well covered, such as Crime or Romance, and some are not as good. \n",
      "\n",
      "The problematic ones are Animation and Comedy\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'The size of the Animation category is: ' + str(genres_count['Animation'])\n",
      "print 'The size of the Comedy category is: ' + str(genres_count['Comedy'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The size of the Animation category is: 6841\n",
        "The size of the Comedy category is: 31758\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, animation just does not have that many movies, but comedy certainly does."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[(x[0],x[1]['tf-idf2']) for x in tf_idf2_stats['Animation'][:4]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "[(u'anim', 7.24), (u'littl', 2.05), (u'off', 1.58), (u'use', 1.49)]"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[(x[0],x[1]['tf-idf2']) for x in tf_idf2_stats['Comedy'][:4]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "[(u'get', 1.38), (u\"n't\", 1.37), (u'so', 1.27), (u'just', 1.25)]"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It seems these categories just have a very unified scattering of words, so it will be very hard to categorize them based on a simple 1-Gram, we should not even try.\n",
      "\n",
      "Maybe we should use 2-Grams or using the titles of the movies. We will get back to this later on.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Calculate the features of a new movie"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create the bag-of-words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bow = set()\n",
      "bow_stem = set()\n",
      "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
      "\n",
      "for genre_stats in genre_key_words.values():    \n",
      "    bow.update([y[0] for y in genre_stats[5]])\n",
      "    bow_stem.update([stemmer.stem(y[0]) for y in genre_stats[5]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(bow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "210"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(bow_stem)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "210"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print bow_stem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set([u'coach', u'help', u'gold', u'show', u'german', u'money', u'over', u'soon', u'alien', u'bring', u'scientist', u'kill', u'gang', u'human', u'erot', u'earth', u'find', u'involv', u'women', u'decid', u'death', u'group', u'sheriff', u'how', u'(', u'father', u'young', u'littl', u'crime', u'offic', u'prison', u'suspect', u'tell', u'save', u'men', u'...', u'them', u'woman', u'return', u'get', u'killer', u'band', u'game', u'were', u'know', u'they', u'fall', u'world', u'now', u'him', u'like', u'tri', u'down', u'magic', u'name', u'brother', u'mysteri', u'these', u'race', u'she', u'rock', u'through', u'marri', u'where', u'ride', u'husband', u'set', u'old', u'her.', u'some', u'play', u'dead', u'sex', u'follow', u'our', u'girl', u'histori', u'sexual', u'out', u\"n't\", u'what', u'detect', u\"'\", u'space', u'boy', u'him.', u'indian', u'leav', u'between', u';', u'we', u'investig', u'power', u'team', u'each', u'footbal', u'evil', u'sing', u'becom', u'come', u'by', u'both', u'perform', u'daughter', u'most', u'seduc', u'howev', u'documentari', u'against', u'turn', u'american', u'win', u'mani', u'murder', u'first', u'there', u'love', u'had', u'into', u'arriv', u'son', u'war.', u'famili', u'news', u'owner', u'use', u'her', u'night', u'stori', u'been', u'fight', u'start', u'secret', u'music', u'includ', u'way', u'armi', u'rancher', u'call', u':', u'was', u'war', u'``', u'himself', u'on', u'forc', u'peopl', u'back', u'must', u'town', u'look', u'hous', u'this', u'outlaw', u'battl', u'up', u'final', u'planet', u'can', u'learn', u'meet', u'danc', u'featur', u'explor', u'then', u'dure', u'year', u'head', u\"''\", u'player', u'believ', u'anim', u'want', u'home', u'crimin', u'nazi', u'seem', u'film', u'!', u'sport', u'soldier', u')', u'anoth', u'song', u'also', u'hor', u'futur', u'which', u'interview', u'adventur', u'begin', u'after', u'discov', u'befor', u'journey', u'plan', u'beauti', u'escap', u'man', u'singer', u'off', u'lead', u'wife', u'ranch', u'life', u'doe', u'polic', u'so', u'mother', u'cattl'])\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def one_gram_features(movie):\n",
      "    stemmer = nltk.stem.snowball.EnglishStemmer()\n",
      "    tokens = [stemmer.stem(token) for token in nltk.word_tokenize(movie['plot'].lower())]\n",
      "    ret_dict = {}\n",
      "    for word in bow:      \n",
      "        ret_dict['has(%s)' %word] = word in tokens\n",
      "    return ret_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[word for word, is_it in one_gram_features(movie).iteritems() if is_it]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "[u'has(stori)',\n",
        " u'has(so)',\n",
        " u'has(sport)',\n",
        " u\"has(')\",\n",
        " u'has(tell)',\n",
        " u'has(show)']"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "movie['plot']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "u\"'Ueberflieger - the art of ski jumping' tells the story of a whole season of ski jumping from the perspective of the top athletes Gregor Schlierenzauer, Thomas Morgenstern and Janne Ahonen. The insider's view shows a very different and so far unknown picture of the sport of ski jumping. \""
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Train the bayes classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify import apply_features\n",
      "\n",
      "feature_set = []\n",
      "L=5000\n",
      "cnt = 0\n",
      "for movie in movies:\n",
      "    if cnt%L == L-1:           \n",
      "        print \"loading line \" + str(cnt)\n",
      "    cnt +=1\n",
      "    for genre in movie['genres']:\n",
      "        features = one_gram_features(movie)\n",
      "        with open('tagged_feature_set.json','a') as fid:\n",
      "            json.dump(features,fid)\n",
      "            fid.write(',' + genre + '\\n')\n",
      "        feature_set.append(())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Dont forget animation, classify it! Maybe COmedy by ommision?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}